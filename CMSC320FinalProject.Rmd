---
title: "Final CMSC320 Project"
output: html_document
---


### Welcome to our tutorial
Written by Sam Jalowicz and Nick Kibbey

# Part 0: Purpose

### What is this project?
This tutorial is written to walk you through the typical data science pipeline.

### Who is this written for?
You! Or anybody interested in data science. This tutorial is written for a beginner level in R. 

### Requirements
Novice coding experience helps and we assume you have been able to use R and have an editor you are comfortable with. Certain statistical concepts may be intimidating if you don't have any background but you'll be fine and can consult the googles. This project is written in R 3.6.0, RMarkdown using RStudio 1.1.463. 


We will be making use of tidyverse and rworldmap and if you do not yet know what tidyverse is I personally recommend you to get aquainted with it at https://www.rdocumentation.org/packages/tidyverse/versions/1.2.1

Rworldmap is used for choropleths (not to be confused with chloroplasts) which is the fancy name for those maps that have geographical boundaries and their assigned color corresponds to some informational attribute. 

Info on this package is surprisingly well articulated at https://cran.r-project.org/web/packages/rworldmap/vignettes/rworldmapFAQ.pdf 


### What will you be doing?
You will be investigating a dataset about number of refugees by year and country of origin.

# Part 1

### Setup
We are using tidyverse and rworldmap when it loads there will probably be warnings and other confusing information that may pop up but there is nothing to worry about if the proceeding code works fine.

```{r setup1, include=FALSE}
library(tidyverse)
library(rworldmap)
```
```{r setup2, eval=FALSE}
library(tidyverse)
library(rworldmap)
```

### Git the data 
__We__ __have__ __provided__ **the** **data** in with this git repo but the original zip is provided by the US state department as a zip folder https://wri-public-data.s3.amazonaws.com/ImpactHack/Refugees.zip

This data will include all the countries the U.S. state department cares to include in their dataset and the amount of refugees from that country by year from 2000 - 2017.

```{r getRefugees}
# CHANGE HERE to if your file is somewhere else
my_refugee_addr <- "Data/aslyum_by_country_origin2.csv"

# Variable that stores the data
refugee_tab <- read_csv(my_refugee_addr)

# Displaying a preview of the data stored in our R program
as_tibble(refugee_tab)
```


### Data Management

Our objective is to be able to make a graph that has time on the X axis and number of refugees on the Y axis but that is difficult with our current data because the year is an attribute and not a value. Our data is too wide and we want it more skinny so that the specific year is a value we can plot and to do that across 17 years we will have 17 entries for each country instead of 1 country with 17 attributes relating to that year's refugees. 

##### TL;DR, We need to modify our table if we want to graph over time

#### How are we doing that?
By using my lazy script to write the R code that creates 17 temp variables that will be merged into one data frame. The whole R code for this task is in the original markdown and the lazy script I used to help me is aptly named lazy.sh.

What you see in the next snippet is the basic gist of what was repeated 17 times with minor adjustments.

```{r dataMod1, include=FALSE}
# Please forgive me but I'm not the one who made a table with 21 attributes when 3 would suffice
# Again I am sorry but sometimes data science makes you do silly parsing 
# You also learn that the smarter way takes longer to do so the stupid way is used more often
temp2000 <- refugee_tab %>%
select(name, yr_2000) %>%
mutate(num_refugees=yr_2000, year=2000)
temp2000$yr_2000 <- NULL

temp2001 <- refugee_tab %>%
select(name, yr_2001) %>%
mutate(num_refugees=yr_2001, year=2001)
temp2001$yr_2001 <- NULL

temp2002 <- refugee_tab %>%
select(name, yr_2002) %>%
mutate(num_refugees=yr_2002, year=2002)
temp2002$yr_2002 <- NULL

temp2003 <- refugee_tab %>%
select(name, yr_2003) %>%
mutate(num_refugees=yr_2003, year=2003)
temp2003$yr_2003 <- NULL

temp2004 <- refugee_tab %>%
select(name, yr_2004) %>%
mutate(num_refugees=yr_2004, year=2004)
temp2004$yr_2004 <- NULL

temp2005 <- refugee_tab %>%
select(name, yr_2005) %>%
mutate(num_refugees=yr_2005, year=2005)
temp2005$yr_2005 <- NULL

temp2006 <- refugee_tab %>%
select(name, yr_2006) %>%
mutate(num_refugees=yr_2006, year=2006)
temp2006$yr_2006 <- NULL

temp2007 <- refugee_tab %>%
select(name, yr_2007) %>%
mutate(num_refugees=yr_2007, year=2007)
temp2007$yr_2007 <- NULL

temp2008 <- refugee_tab %>%
select(name, yr_2008) %>%
mutate(num_refugees=yr_2008, year=2008)
temp2008$yr_2008 <- NULL

temp2009 <- refugee_tab %>%
select(name, yr_2009) %>%
mutate(num_refugees=yr_2009, year=2009)
temp2009$yr_2009 <- NULL

temp2010 <- refugee_tab %>%
select(name, yr_2010) %>%
mutate(num_refugees=yr_2010, year=2010)
temp2010$yr_2010 <- NULL

temp2011 <- refugee_tab %>%
select(name, yr_2011) %>%
mutate(num_refugees=yr_2011, year=2011)
temp2011$yr_2011 <- NULL

temp2012 <- refugee_tab %>%
select(name, yr_2012) %>%
mutate(num_refugees=yr_2012, year=2012)
temp2012$yr_2012 <- NULL

temp2013 <- refugee_tab %>%
select(name, yr_2013) %>%
mutate(num_refugees=yr_2013, year=2013)
temp2013$yr_2013 <- NULL

temp2014 <- refugee_tab %>%
select(name, yr_2014) %>%
mutate(num_refugees=yr_2014, year=2014)
temp2014$yr_2014 <- NULL

temp2015 <- refugee_tab %>%
select(name, yr_2015) %>%
mutate(num_refugees=yr_2015, year=2015)
temp2015$yr_2015 <- NULL

temp2016 <- refugee_tab %>%
select(name, yr_2016) %>%
mutate(num_refugees=yr_2016, year=2016)
temp2016$yr_2016 <- NULL

temp2017 <- refugee_tab %>%
select(name, yr_2017) %>%
mutate(num_refugees=yr_2017, year=2017)
temp2017$yr_2017 <- NULL
```


```{r dataMod2, eval=FALSE}
# Please forgive me but I'm not the one who made a table with 21 attributes when 3 would suffice
# Again I am sorry but sometimes data science makes you do silly parsing 
# You also learn that the smarter way takes longer to do so the stupid way is used more often
temp2000 <- refugee_tab %>%
  select(name, yr_2000) %>%
  mutate(num_refugees=yr_2000, year=2000)
temp2000$yr_2000 <- NULL

```

This is the code that will bind all our temp variables together into one tall data frame.

```{r dataMod3}
refugee_df <- bind_rows(temp2017, temp2016, temp2015, temp2014, temp2013, temp2012, temp2011,
                        temp2010, temp2009, temp2008, temp2007, temp2006, temp2005, temp2004,
                        temp2003, temp2002, temp2001, temp2000)
as_tibble(refugee_df)

```

Now we will load in our population set. 

This is in the Data folder and originally sourced from https://www.kaggle.com/szakwani/worldpopulation/version/1

We only care about the year and population columns, so we remove the rest. These columns are initially set to chr attributes, so in order to work with them later on, we need to convert them. We convert population and year into numerics. This will work out fine if you use our provided csv but the original kaggle kernel has a copyright symbol in the final row that unfortunately makes the type conversion upset when you knit to html but works in console (you may also remove it through regex if manual isn't your style).

```{r pop}
population_df <- read_csv("Data/worldPop.csv") # read data

# clear out the unnecessary attributes
population_df$`average annual` <- NULL
population_df$`average annual_1` <- NULL

# convert population, year into numerics 
# (gsub regex makes 1,699,224 into 1699224 for better conversion)
population_df$population <- as.numeric(gsub(",","", population_df$population))
population_df$year <- as.numeric(population_df$year)

# extract years 2000 - 2017 matching our refugee dataset
population_df <- population_df %>% 
  group_by(year) %>%
  select(year, population) %>%
  filter(year >= 2000, year <= 2017)

as_tibble(population_df)
```


Now we are going to go a step forward to make a new dataframe that has both world totals of both population and refugees, as well as handy column that gives us the percent of world population of refugees.

```{r world}
# data frame that has yearly sums of refugees
ref_totals <- refugee_df %>%
  group_by(year) %>%
  select(year, num_refugees) %>%
  summarize(num_refugees=sum(num_refugees, na.rm = TRUE)) 

# data frame that has years sums of our additional info
world_totals <- ref_totals %>%
  left_join(population_df) %>%
  mutate(ref_portion=(num_refugees/population))

as_tibble(world_totals)

```


Now isn't that better?

# Part 2: Exploratory data analysis

In order to get a better sense of the data we're working with, we perform exploratory data analysis. We try to manipulate the data to see if there are trends that we can work with, or see if there any correlations we can identify.

Here we are creating a map of refugee data around the world, to get a better picture of the distribution of refugee populations. The areas in yellow have lower numbers of refugees, while the areas in orange have more, and red has the highest. We did this by utilizing the rworldmap library. From this map you can see areas of interest in 2010, and can try to research what was going on in specific countries in 2010 that may have led to these statistics.

```{r chloroplast}
# 2010 chosen as an arbitrary year to demonstrate choropleth
refugees2010 <- refugee_df %>%
  filter(year==2010) 
  
# library from rworldmap matches our names to the library with associated data
ref_map <- joinCountryData2Map(refugees2010, joinCode = "NAME", nameJoinColumn = "name")

# plot the map
mapParams <- mapCountryData(ref_map, nameColumnToPlot='num_refugees')
```

Here we are exploring if there was a realtionship between time and the total number of refugees. We are using the dataframe we created in the first section to help us do this. We are plotting refugees over time, and using a regression line in order to see the general trend over time.

```{r totalRefGraph}

ref_totals %>%
  ggplot(mapping=aes(x=year, y=num_refugees)) +
    geom_smooth(method=lm) +
    geom_point() + labs(title="Number of Total Refugees Over Time",
          x="Year", y="Number of Refugees")

```

To get a bit of perspective we are plotting the world population growth to compare.


We are plotting the total population for each year, and a regression line to this graph as well. As you can see here, this data appears to be more consistent over time than the refugee data that we just plotted.

```{r explore2}
population_df %>%
  filter(year >= 2000,year <= 2017) %>%
  ggplot(mapping=aes(x=year, y=population)) +
    geom_smooth(method=lm) +
    geom_point() + labs(title="Total World Population Over Time",
          x="Year", y="Population")
```

Both world population and refugee totals seem to be growing so the relative percentages are also important to view in order to find any interesting relationships. 

```{r portionRefGraph}
world_totals %>%
  ggplot(mapping=aes(x=year, y=ref_portion)) +
    geom_smooth(method=lm) +
    geom_point() + labs(title="Portion of world population refugees over time ",
          x="Year", y="Percent of World's population as Refugees")
```

From the first 2 graphs we could see numbers were increasing but with this additional graph it definitely appears that the relative portion seems to be increasing which is an intuition we'll want to test later on.

***

We also want to explore trends in individual countries over time. From looking at the initial data, we saw a decrease in Angola refugees, and wanted to explore that. We are plotting the data, and adding a regression line.

```{r angola}
refugee_df %>%
  filter(name == "Angola") %>%
  ggplot(mapping=aes(x=year, y=num_refugees)) +
    geom_smooth(method=lm) +
    geom_point() + labs(title="Number of Refugees in Angola Over Time",
          x="Year", y="Number of Refugees")
```

We are going a step further and breaking it down into four distinct time periods. This can be useful when trying to figure out the underlying causes in the refugee decrease. You can see the steepest decrease in the 2000-2004 time period, which may be due to the fact that it was when Angolas 30 year civil war was coming to an end, and the Angolan govenment began assisting those that were displaced.

Source: https://www.unhcr.org/en-us/news/latest/2007/3/4607b7d24/repatriation-angola-officially-ends-410000-refugees-home.html

```{r angola2}
refugee_df %>%
  filter(name == "Angola") %>%
  mutate(cut_yearID = cut(x=year,breaks=4)) %>%
  group_by(cut_yearID, name) %>%
  rowid_to_column() %>%
  ggplot(mapping=aes(x=year, y=num_refugees)) +  
    facet_wrap(~cut_yearID) +
    geom_point() +
    geom_smooth(method=lm)
```

From here we want to look at a country that saw an increase in refugees, and try to figure out why this may be the case. This example plots the data for Syria with a regression line.

```{r syria}
refugee_df %>%
  filter(name == "Syria") %>%
  ggplot(mapping=aes(x=year, y=num_refugees)) +
    geom_smooth(method=lm) +
    geom_point() + labs(title="Number of Refugees in Syria Over Time",
          x="Year", y="Number of Refugees")
```

We are breaking this one down into four parts as well, and the results did not disappoint. You can see relatively consistent numbers in the first two graphs, which contain data from 2000-2008. In the third one you can see that in 2012 there was a steep jump in the number of refugees, which gives that regression line a greater slope. In the fourth graph you see a steady increase over the years, which is why the initial graph above shows an increase in the number of refugees in Syria over time. Upon further research, 2011/2012 is when the Syrian Civil War began. As with the Angola data, we are starting to see a pattern. It's starting to look like many of these refugees can be attributed to times of civil war and distress in individual countries. 

Source: https://www.worldvision.org/refugees-news-stories/syrian-refugee-crisis-facts

```{r syria2}
refugee_df %>%
  filter(name == "Syria") %>%
  mutate(cut_yearID = cut(x=year,breaks=4)) %>%
  group_by(cut_yearID, name) %>%
  rowid_to_column() %>%
  ggplot(mapping=aes(x=year, y=num_refugees)) +  
    facet_wrap(~cut_yearID) +
    geom_point() +
    geom_smooth(method=lm)
```

In order to put it in perspective, we are plotting the trends on the same graph. While Angola may look like it has a steep decrease on its own, it looks relatively consistet on a larger scale when compared to the increase in Syrian refugees.

```{r explore4}
refugee_df %>%
  filter(name == "Angola" | name == "Syria") %>%
  ggplot(mapping=aes(x=year, y=num_refugees, color=name)) +
    geom_smooth(method=lm) +
    geom_point() + labs(title="Number of Refugees in Select Countries Over Time",
          x="Year", y="Number of Refugees")
```

# Part 3: Hypothesis Testing and Machine Learning

In order to figure out how "good" our intuitions are, we perform hypothesis testing. We create a hypothesis and perform different tests to see how confident our results are. You can either reject the null hypothesis or fail to reject the null hypothesis. We want to examine the rate of growth of refugees.

Our Hypothesis: Refugee populations are growing at a faster rate than the world population

***

To start off we will find out from year to year what the rate of refugees/year is growing or shrinking.

We will use linear regression to get a sense of the linear relationship between time and refugee population.

First we use the cor() function to get an indication if number of refugees is dependent on year with 1 being completely positivly dependent, -1 being inversely, and 0 indicating no correlation.

```{r correlation1}
# look to see if there appears to be a correlation
cor(ref_totals$year, ref_totals$num_refugees)
```

0.93 is a definite correlation between the two and now we move on to making or linear regression model with the lm function to analyze the annual change in number of refugees. The summary function was included to show a larger array of stats but for our purposes the tidy values are all we need and a lot more readable.


```{r hypothesis1}

# creating a linear model for our total refugees/year data
ref_lm <- lm(num_refugees~year, data=ref_totals)

# see a full summary
summary(ref_lm)

# more human readable format
ref_values <- ref_lm %>%  
  broom::tidy()
ref_values

```


This means from 2000-2017 we estimate that there is an increase of 2,822,673 refugees per year. We can trust this statistic because our p value (1.94e-08) < 0.05, which means we are confident that this estimate didn't occur by coincidence. In statistical terms we reject the null hypothesis that there is no relationship between the year and total refugees within our dataset because our p value was less than our alpha value of 0.05 which is the standard.


2,822,673 more refugees per year is a big number but there are a lot of people in the world so it is hard to make sense of this statistic.

That is why we included that whole thing about the world population so that we could see if the relative distribution if people who are refugees in the world is also increasing...

So now we will start testing to see if the portion of the world's refugee population is growing faster than world's overall population growth.

```{r hypothesis2}

# creating a linear model for our total refugees/year data
world_lm <- lm(ref_portion~year, data=world_totals)
world_values <- world_lm %>%  
  broom::tidy()
world_values

```

This shows us that our thinking was correct. There is an approximate 0.035% increase in the portion of the world's population that are refugees, which means that refugee growth is outpacing world population growth. Looking at our estimate, this margin of difference is small, but still significant as our p value is < 0.05, and considering that the refugee proportion of the world in 2017 (its highest in our dataset) was 0.9% this increase does matter. Comparing numbers of different magnitudes like 7 billion people and 2.8 million more refugees per year is a difficult interpretation, so looking at percents help put this number in perspective.

***

### Predicting Linear Regression Model

Now normally in data science we also are looking to test that the linear regression model we built would be good if we introduced new data sets to predict the error we might expect. This may seem odd for this dataset because without getting too philosphical, in theory there is an absolute number for the amount of people alive in 2000 and the amout of refugees as there is one earth with humans living on it in 2000 so why would we predict error for variation that don't exist. It's a good point but good data scientists are thorough and skeptical and its perfectly reasonable a scenario exists that the reporting of this data from dataset to dataset could be different for a number of reasons from countries poorly communicating with the U.S. state department to human error.

##### TL;DR data scientists are thorough so we are going to predict how good our model is

We are using 80:20 rule, which is a popular standard, to set out 20% of our dataset and train a model on the other 80%. We see how well the model performs and can look at the error for our predictions based on our model.

```{r predict}

# setting seed
set.seed(1234)  
# row indices for training data
trainingRowIndex <- sample(1:nrow(world_totals), 0.8*nrow(world_totals))  
trainingData <- world_totals[trainingRowIndex, ]  # model training data
testData  <- world_totals[-trainingRowIndex, ]   # test data

```

so now we got ourselves training and test data so it is time to build a model and look at our error.

```{r predict2}


lm_mod_total <- lm(num_refugees ~ year, data=trainingData)  # build the model
ref_pred <- predict(lm_mod_total, testData)  # predict 
summary (lm_mod_total)  # model summary

```

Remember our full model estimate was 2.8 mil, while the model on 80% of the data is estimating 3.3 mil refugees/year and the p value is also < 0.05. This all makes sense that our new model within range of sensible expectation and it's good to do these type of sanity checks throughout the data science pipeline.

Now we are binding our prediction data and our actual data values and seeing if we indeed have a strong correlation.

```{r errors}
actual_p <- data.frame(cbind(actuals=testData$num_refugees, predicteds=ref_pred))  
cor(actual_p)  # find correlation between actual values and those predicted
actual_p # display data for you

```



And just for comparison in one code block we are also going to look at that world porportion statistic.


```{r predicting}

# setting seed
set.seed(1234)  

lm_mod_portion <- lm(ref_portion ~ year, data=trainingData)  # build the model
portion_pred <- predict(lm_mod_portion, testData)  # predict 
summary(lm_mod_portion)  # model summary


actual_p2 <- data.frame(cbind(actuals=testData$ref_portion, predicteds=portion_pred))  
cor(actual_p2)  # find correlation between actual values and those predicted
actual_p2 # display data for you
```


From both of these models we have learned to have a bit of trust when it concerns some of the analysis that was done. We also learned a depressing fact that despite having no major world war type of scenarios we are seeing a greater increase in our population being refugees. 

Another dataset that we were thinking about using is kindly available at https://www.acleddata.com/data/ and they also have a lot of visualizations tools relating to by country conflict data. This is a really cool dataset but we wanted to test something like whats the correlation of conflicts/country and refugees/country but it seemed a bit obvious since conflict would be a criterion to be officially seen as a refugee.

The whole repository is available at https://github.com/nkibbey/nkibbey.github.io

I hope you enjoyed our tutorial.